---
title: "Module 6: Linear models II"
author: "Jake Ferguson"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
header-includes :
  - \usepackage {amsmath}
  - \usepackage{mathrsfs}
---
  
```{r, echo=F, message=F, warning=F, eval=T}
#setting up my ggplot defaults. Update with your own preferences
library(ggplot2)#plotting functions
library(ggthemes) #more themes!
library(wesanderson)
library(RColorBrewer)

theme_set(theme_tufte()) # a theme I like.
theme_update(plot.title = element_text(hjust = 0.5), 
             axis.line.x = element_line(color="black", size = 1),
             axis.line.y = element_line(color="black", size = 1),
             text=element_text(size=20),
             axis.text=element_text(size=15)) #center all titles and and axis lines
```

# Module goals

<br>
  
1. 

---
# Collinearity versus multicollinearity

<br>

**Collinearity** - when one predictor variable, $X_1$, is correlated with another, $X_2$

**Multicollinearity** - when multiple independent variables are correlated with each other.



.footnote[Graham 2003. Confronting multicollinearity in ecological multiple
regression. Ecology 84:2809-2815.]

---
# Examples of collinear predictors

- A person's height and weight

- Monthly average temperature and max/min temperatures

- Mean annual temperature and annual rainfall.

<br>

Examples from your study systems?

---
# Type of collinearity

- Variables that are collinear and each have their own separate “effect” on the response variable, Y
  - Tigers avoid areas near human settlements and also areas
with domestic animals
  - Domestic animals tend to be found near human settlements
  
  
- **Redundant** predictors have essentially the same meaning.
  - various morphometric measures of body size (lengths, masses, ratios, areas)

- **Compositional** variables have to sum to 1 (because last category is determined by others)
  - percent cover of habitat types

---
# Symptoms of predictor collinearity

- Variables may be significant in simple linear regression, but not in multiple regression

- Large standard errors in multiple regression models

- Large changes in coefficient estimates between full and reduced models

---
# Variance Inflation Factors

Get the $R^2$ coefficient for variable $x_j$ as a function of other predictors:

\begin{align*}
R^2_{x_j} =  \mathrm{lm}(x_j \sim x_1 + x_2 + \ldots + x_{j-1} + x_{j+1} + \ldots +x_k) 
\end{align*}

Multicollinearity is then measured using the **variance inflation factor** (VIF): 

$$VIF(\hat{\beta_j}) = \frac{1}{1-R^2_{x_j}}$$

Can be calculated using `vif` in the `car` package.

General rules of thumb:
- Many suggest VIFs $>$ 10 are problematic
- In my experience VIF's $>$ 3 can cause issues

---
# Potential consequences

Models with collinear variables have: 

- inflated standard errors

- misleading estimates of effect

- coefficients that are difficult to interpret

---
#Example:  Monet

.pull-left[
<br>

```{r, echo=F}
monet.dat <- read.csv(file="../Data/monet.csv", header=TRUE)
monet.dat$HOUSE <- as.factor(monet.dat$HOUSE)
monet.dat$SIGNED <- as.factor(monet.dat$SIGNED)
monet.dat <- monet.dat[,-5]
head(monet.dat)
```
]

.pull-right[
<br>

```{r, echo=F, out.width="85%", fig.align="right"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/3/3c/Claude_Monet_-_The_Cliff_of_Aval%2C_Etr%C3%A9tat_-_Google_Art_Project.jpg")
```

.footnote[image: [Google Art Project](https://upload.wikimedia.org/wikipedia/commons/3/3c/Claude_Monet_-_The_Cliff_of_Aval%2C_Etr%C3%A9tat_-_Google_Art_Project.jpg)]
]

---
# VIFs

```{r, echo=F, eval=F}
full.mod <- lm(Response ~ OD + BD + LTD + W, data=graham.dat)
#not currently used
```

```{r, warning=F, message=F}
library(car)
full.mod <- lm(log(PRICE) ~ HEIGHT + WIDTH + SIZE, data=monet.dat)

vif(full.mod)
```

<br>

What to do now?

---
class: center, middle, inverse

# Handling multicollinearity

---

#  Strategies for Handling Confounding Variables

Consider goals: if **prediction** is your goal collinearity may not be a problem.

<br>

If **understanding** predictor effects is important, consider removing one more variables:

```{r}
nosize.mod <- lm(log(PRICE) ~ HEIGHT + WIDTH, data=monet.dat)
vif(nosize.mod)
```

---
class: inverse, middle

# Excercise 6

## Examining multicollinearity

---
# Other techniques to handle multicollinearity

- Create new predictors with Principle Components Analysis or Factor analysis

- Ridge regression: gives better estimates of effects and SE's

- Structural equation modeling 

```{r, echo=F, fig.cap="http://jarrettbyrnes.info", fig.align="center"}
knitr::include_graphics("http://jarrettbyrnes.info/pics/simple_path.png")
```

---
# 

---
class: inverse, middle

# Excercise 6

## Examining multicollinearity

