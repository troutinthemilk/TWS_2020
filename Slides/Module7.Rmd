---
title: "Module 7: Generalized linear models"
author: "Jake Ferguson"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
header-includes :
  - \usepackage {amsmath}
  - \usepackage{mathrsfs}
---
  
```{r, echo=F, message=F, warning=F, eval=T}
#setting up my ggplot defaults. Update with your own preferences
library(ggplot2)#plotting functions
library(ggthemes) #more themes!
library(wesanderson)
library(RColorBrewer)

theme_set(theme_tufte()) # a theme I like.
theme_update(plot.title = element_text(hjust = 0.5), 
             axis.line.x = element_line(color="black", size = 1),
             axis.line.y = element_line(color="black", size = 1),
             text=element_text(size=20),
             axis.text=element_text(size=15)) #center all titles and and axis lines
```



# Module goals

<br>

<br>

1. Generalize regression to count and categorical data

2. Identify proper distribution to use for particular datasets

3. Evaluate model quality

---
# Linear regression

<br>

### The lm framework: 

\begin{align*}
\mu &= \beta_0 + \beta_1\cdot x_1 + \dots + \beta_p x_p \\
Y &\sim N(\mu,\, \sigma^2)\\
\end{align*}

We assume that the residuals are normally distributed around the mean, $\mu$.

---
# Limitations of `lm`

<br>

Although linear models are a very useful framework, there are some situations where they are not appropriate

- the range of $Y$ is restricted (e.g. binary, count)

- the variance of $Y$ depends on the mean

<br>

--

Generalized linear models (glms) extend the linear model framework to address both of these issues

---
# Generalized linear regression

In a glm there is some transformation of the mean, $g(\mu)$, called the **link** function, that results in a linear model.

\begin{align*}
g(\mu) &= \beta_0 + \beta_1\cdot x_1 + \dots + \beta_p x_p \\
\mu &= g^{-1}\left(\beta_0 + \beta_1\cdot x_1 + \dots + \beta_p x_p\right) \\
Y &\sim f(\mu,\, \theta)
\end{align*}

We assume that the residuals are distributed around the mean, $\mu$, following some distribution $f(\cdot)$ (e.g., Binomial, Poisson, Negative Binomial). Where $\theta$ is any relevant additional parameters needed to model the  variance. 

<br> 

By choosing an appropriate **link** function, $g( \cdot )$, we will ensure that the mean only takes on values that are supported by the distribution (for example only positive values for the Poisson or between 0 and 1 for the Binomial).


---
# Modeling counts: Poisson regression

<br>

Use the log-link function: $g(\mu) = ln(\mu)$. Then the the inverse links is $e^\cdot$.


\begin{align*}
  \mu &= e^{\beta_0 + \beta_1\cdot x_1 + \dots + \beta_p x_p} \\
  Y &\sim \mathrm{Poisson}(\mu)
\end{align*}

$$ E[Y] = \mu, \,\,\,\,\,\,\,\, Var[Y]=\mu $$

```{r, out.width="60%", fig.asp=0.6, echo=F}
xseq <- seq(-5, 5, length.out=1000)
mu <- exp(xseq)
ggplot() +
  geom_line(aes(x=xseq, y=mu), size=1.3) + xlab("x") + ylab(expression(e^x)) + geom_rangeframe(aes(x=xseq, y=mu))
```

---
# Continuous predictor variables

```{r, echo=F, out.width="50%", fig.align="center"}
set.seed(1)
x<-seq(0,100,1) 
mus<-exp(0.01+0.03*x) # mean at each x
y<-rpois(length(x),lambda= mus) # 1 observation per x

ggplot(data=data.frame(x, y), aes(x=x, y=y)) + geom_point(size=2) + xlab("% Cover Grass") + ylab("Number of pheasants") + geom_rangeframe()
```


\begin{align*}
  \mu &= e^{0.01 + 0.03\cdot X} \\
  Y &\sim \mathrm{Poisson}(\mu)
\end{align*}

---
# Using `glm`

The syntax for `glm` follows `lm` closely:

```{r, eval=F}
glm(RESPONSE ~ X1 + X2, data=data.csv, family=poisson(link="log"))
```

We've added the `family` argument to specify which distribution to use. We've also added `(link="log")` to specify the link function we use. 

<br>

----

<br>
  
This website has a nice table of families and their link functions in `glm`: https://data.princeton.edu/R/GLMs

---
# `glm` model output

```{r, echo=F}
pheas.dat <- data.frame(grassCover=x, Pheasants=y)
```

```{r}
summary(glm(Pheasants ~ grassCover, data=pheas.dat, family=poisson))
```

---
# How to interpret $\beta_0$ and $\beta_1$?
```{r}
coef(glm(Pheasants ~ grassCover, data=pheas.dat, family=poisson))
```

--

<br>

----
.pull-left[
On the **link** scale parameters mean the same as in `lm`. 
$$\ln(\mu) = \beta_0 + \beta_1 X$$

$\beta_0$: intercept of the log-mean 

$\beta_1$: slope of the log-mean
]
--

.pull-right[
On the **natural** scale they may mean something different:

\begin{align*}
  \mu &= e^{\beta_0 + \beta_1 X} \\
  \mu &= e^{\beta_0}\cdot e^{\beta_1 X} 
\end{align*}

$\beta_0$: mean log-fecundity $at X=0$

$\beta_1$: rate of change in fecundity with $X$
]
---
# Deviance


`    Null deviance: 484.156  on 100  degrees of freedom`

`Residual deviance:  94.911  on  99  degrees of freedom`


<br> 

**Deviance** is a measure of model fit.

**Null deviance** is the difference in fit between the saturated model and the intercept model

- the *saturated* model has a parameter at each observation 

- the intercept only has a single parameter 


**Residual deviance** is the difference in fit between the saturated model and the fitted model


---
# Why Deviance?

There is no $R^2$ for glm's typically so we can instead use the proportion deviance explained (called the pseudo $R^2$).

$$R^2_\mathrm{pseudo} = 1 - \frac{\mathrm{Residual\,\, deviance}}{\mathrm{Null\,\, deviance}}$$
- **Null deviance** $\approx$ equivalent of total sum of squares.

- **Residual deviance** $\approx$quivalent of residual sum of squares.

--

<br>

For the pheasant model, $R^2_{pseudo}=$ `r round(1 - 94.911/484.156,2)`.
---
class: inverse, middle

# Exercise 7A
-----
## Fit a count model

---
# Offsets in Poisson regression

Count data ( $Y$) are often collected:

- over varying lengths of time
- in sample units that have different areas

<br>

So we are often interested in modeling rates:

\begin{align*}
Y/\mathrm{time}
\end{align*}

or densities:

\begin{align*}
Y/\mathrm{Area}
\end{align*}

---
# Modeling beaver densities in MN

Each route is a different length.

```{r, out.width="60%", echo=F, fig.align="center"}
knitr::include_graphics("BeavMap.jpg")
```

---
# Beaver density `glm`

Using route length as an **offset** controls for differences in survey effort. 

```{r, echo=F}
beav.dat <- read.csv(file="../Data/ColCountsFinal.csv")
#beav.dat <- subset(beav.dat, rte.name == "Blackduck" | rte.name == "Kabetogama" | rte.name == "Kanabec" | rte.name == "Cass" | rte.name == "Itasca")
beaver.glm <- glm(num.col ~ rte.name + offset(log(rte.km)), data=beav.dat, family=poisson)

summary(beaver.glm)
```

---
# Model selection

### Can use same tools as linear regression. 

<br>

- t-tests on parameter estimates

- Confidence intervals (`confint`)

- ANOVA (f-tests)

- AIC


---
# Is the Poisson suitable?

<br>

An important assumption in the Poisson distribution is that the mean and variance are equal: $\mathrm{E}[Y] = \mathrm{Var}[Y]$.

<br>

How can we test this assumption?

- Examine residual plots

- Formal goodness of fit tests exist (Pearson's $\chi^2$, )

- Fit model with overdispersion (next section) and compare via AIC

---
# Overdispersion

**Reasons** data may be overdispersed

- Omitted variables

- Measurement error

- Wrong distribution


**Consequences** of overdispersion

- Standard errors may underestimated

- More complex models than necessary may be selected


---
# Negative Binomial

The true variance is often higher (**overdispersion**) than the Poisson distribution. The Negative Binomial distribution allows for this overdispersion.

\begin{align*}
\mathrm{E}[y] &= \mu\\
\mathrm{Var}[y] &= \mu + \mu^2/\theta
\end{align*}

```{r, echo=F, eval=T, out.width="70%", fig.asp=0.7}
x <- 0:12
plot(x, dpois(x=x, lambda=5), type='l',  lwd=2, ylab="Probability")
lines(x, dnbinom(x=x, mu=5, size=2), col="orange", lwd=2)
lines(x, dnbinom(x=x, mu=5, size=10), col="cornflowerblue", lwd=2)
```

---

# Negative binomial regression

```{r}
library(MASS)
library(MuMIn)
beaver.pois <- glm(num.col ~ rte.name + offset(log(rte.km)), data=beav.dat, family=poisson)
beaver.nb <- glm.nb(num.col ~ rte.name + offset(log(rte.km)), data=beav.dat)

model.sel(list(beaver.pois, beaver.nb))
```

Based on this output is **overdispersion** present?
---
# `glm.nb` output

```{r}
beaver.nb
```

---
class: inverse, middle

# Exercise 7B
-----
## Negative binomial regression

---

# Modeling categorgies: Binomial (logistic) regression 


We've covered: 

1. Continuous response, continuous predictor (regression)

2. Continuous response, discrete predictor (t-test, ANOVA)

<br>

Binomial regression has a **discrete response** with continuous or discrete predictors.


<br>

Examples: survival, annual recruitment, presence/absence, disease infection/recovery

---
# Logistic regression

\begin{align*}
\mathrm{logit}(p) &\equiv \ln\left( \frac{p}{1-p}\right) = \beta_0 + \beta_1\cdot x_1 + \dots + \beta_p x_p\\
p &= \frac{e^{\beta_0 + \beta_1\cdot x_1 + \dots + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1\cdot x_1 + \dots + \beta_p x_p}} \\
Y &\sim \mathrm{binomial}(N,\, p)
\end{align*}

<br>

#### The binomial distribution properties:

\begin{align*}
E[Y] &= Np\\
Var[Y] &= Np(1-p)
\end{align*}



---
#Odds

$\frac{p}{1-p}$ is called the **odds** and gives the relative probability of success.  It is often used in betting. 

.pull-left[
<br>

```{r, echo=F}
p <- c(0.1, 0.25, 0.5, 0.75, 0.9)
odds <- p/(1-p)
log.odds <- log(odds)
knitr::kable(data.frame(p, "odds"=round(odds,2), "logit(log odds)"=round(log.odds,2)), 'html')
```
]

.pull-right[
```{r, out.width="100%", fig.asp=1.1, echo=F}
xseq <- seq(-5, 5, length.out=1000)
p <- exp(xseq)/(1 + exp(xseq))
ggplot() +
  geom_line(aes(x=xseq, y=p), size=1.3) + xlab("x") + ylab(expression(e^x/(1+e^x))) + geom_rangeframe(aes(x=xseq, y=p))
```
]

---
# Interpreting parameters in logistic regression

For a continuous predictor variable, $x_1$ , the regression coefficient, $\beta_1$ , represents the change in log-odds per unit change in $x_1$ holding other predictors constant.

```{r, out.width="100%", fig.asp=0.4, echo=F}
library(gridExtra)
xseq <- seq(-5, 5, length.out=1000)
p <- exp(xseq)/(1 + exp(xseq))
p2 <- exp(-xseq)/(1 + exp(-xseq))
p3 <- exp(0.5*xseq)/(1 + exp(0.5*xseq))

plot1 <- ggplot() +
  geom_line(aes(x=xseq, y=p), size=1.3) +
  geom_line(aes(x=xseq, y=p2), col="cornflowerblue", size=1.3) +
  xlab("x") + ylab("Probability") + geom_rangeframe(aes(x=xseq, y=p)) +
  ggtitle("Change in sign")

plot2 <- ggplot() +
  geom_line(aes(x=xseq, y=p), size=1.3) +
  geom_line(aes(x=xseq, y=p3), col="cornflowerblue", size=1.3) +
  xlab("x") + ylab("Probability") + geom_rangeframe(aes(x=xseq, y=p)) + 
  ggtitle("Change in magnitude")

grid.arrange(plot1, plot2, nrow = 1)
```

---
# Interpreting parameters in logistic regression


The intercept determines the probability at $x=0$: $p=\frac{e^{\beta_0}}{1 + e^{\beta_0}}=?$



```{r, out.width="60%", fig.asp=0.5, echo=F, fig.align="center"}
p <- exp(xseq)/(1 + exp(xseq))
p2 <- exp(2+xseq)/(1 + exp(2+xseq))

ggplot() +
  geom_line(aes(x=xseq, y=p), size=1.3) +
  geom_line(aes(x=xseq, y=p2), col="cornflowerblue", size=1.3) +
  xlab("x") + ylab("Probability") + geom_rangeframe(aes(x=xseq, y=p)) +
  ggtitle("Changing intercept")

```


---
# Response data in logistic regression

We can have response data of 0 or 1...

```{r, eval=F}
glm.fit  <- glm(y ~ x, data=sim.dat, family=binomial)

```

.pull-left[

```{r, echo=F}
set.seed(1)
x        <- rnorm(20, 0, 10)
y        <- rbinom(length(x), size=1, p=exp(1 + 0.1*x)/(1 + exp(1 + 0.1*x)))
sim.dat  <- data.frame(x=x, y=y)
glm.fit  <- glm(y ~ x, data=sim.dat, family=binomial)

pred.x   <- seq(-30, 30, length.out=100)
pred.sim <- predict(glm.fit, newdata=data.frame(x=pred.x), type="response")
new.dat  <- data.frame(x=pred.x, y=pred.sim)
knitr::kable(sim.dat[1:9,], format="html")
```
]

.pull-right[

```{r, echo=F}
ggplot(data=sim.dat, aes(x=x, y=y)) +
  geom_point(size=3) + 
  geom_line(data=new.dat, aes(x=x, y=y)) + 
  geom_rangeframe() + xlab("X") + ylab("Y")
```
]

---
# Response data in logistic regression II

... or the response data can be a vector of successes and failures

```{r, eval=F}
glm(cbind(successes, fails) ~ x, data=sim.dat, family=binomial)
```

.pull-left[
```{r, echo=F}
set.seed(1)
x        <- rnorm(20, 0, 10)
n        <- rpois(20, 20)
y        <- rbinom(length(x), size=n, p=exp(1 - 0.1*x)/(1 + exp(1 - 0.1*x)))
sim.dat  <- data.frame(successes=y, fails=n-y, x=x)
knitr::kable(sim.dat[1:9,], format="html")
```
]

.pull-right[
```{r, echo=F}
glm.fit  <- glm(cbind(successes, fails) ~ x, data=sim.dat, family=binomial)
```

```{r, echo=F}
pred.x   <- seq(-30, 30, length.out=100)
pred.sim <- predict(glm.fit, newdata=data.frame(x=pred.x), type="response")
new.dat  <- data.frame(x=pred.x, y=pred.sim)

ggplot(data=sim.dat, aes(x=x, y=successes/(successes+fails))) +
  geom_point(size=3) + 
  geom_line(data=new.dat, aes(x=x, y=y)) + 
  geom_rangeframe() + xlab("X") + ylab("Frequency of success")
```
]

---
# Example: Haleakalā silverswords


```{r, echo=F, out.width="34%", fig.align="center"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/4/4f/Argyroxiphium_sandwicense_subsp._macrocephalum.png")
```

  
  
.footnote[Krushelnycky, P.D., et al, 2013. Climate‐associated population declines reverse recovery and threaten future of an iconic high‐elevation plant. Global Change Biology, 19(3)

Image: wikimedia]

---
# Survival

```{r, echo=FALSE, out.width="60%", fig.align="center"}
silver.dat <- read.csv(file="../Data/Silversword.csv")
silver.dat <- silver.dat[order(silver.dat$Elev),]
silver.dat$Survival <- silver.dat$Survive/silver.dat$Pop_size

ggplot(data=silver.dat, aes(x=Elev, y=Survival)) +
  geom_point(size=3) + 
  geom_rangeframe() + 
  xlab("Elevation") + ylab('Proportion surviving')
```

---
# Modeling survival with `glm`

```{r, echo=F}
mod.sil <- glm(cbind(Survive, Die) ~ Elev, data=silver.dat, family=binomial)
print(summary(mod.sil))
pred.sil <- predict(mod.sil, newdata=silver.dat, type="response")

```


---
# How did we do?


$R^2_{pseudo}=$ `r 1 - round(794/2074, 2)`

```{r, out.width="60%", echo=F, fig.align="center"}
ggplot(data=silver.dat, aes(x=Elev, y=Survival)) +
  geom_point(size=3) + 
  geom_rangeframe() + 
  geom_line(y=pred.sil) + 
  xlab("Elevation") + ylab('Proportion surviving')

```

---
class: inverse, middle

# Exercise 7C
-----
## Modeling categorical data

---
# What about overdispersion?

The beta-binomial distribution allows for overdispersion
$$
\begin{align*}
E[Y] &= N \mu, \\ 
Var[Y] &= N \mu \left(1 - \mu \right) + \rho N(N-1) \mu \left(1 - \mu \right)
\end{align*}
$$
```{r, warning=F, message=F, echo=F, out.width="60%", fig.asp=0.8}

x <- 0:10

plot(x, dbinom(x=x, size=10, prob=0.3), type='l', lwd=2, xlab="X", ylab="Density")
lines(x, rmutil::dbetabinom(y=x, size=10, m=0.3, s=20), lwd=2, col='orange')
lines(x, rmutil::dbetabinom(y=x, size=10, m=0.3, s=5), lwd=2, col='cornflowerblue')


```

---
# Beta-binomial regression

```{r, warning=F, message=F}
library(VGAM) #contains beta-binomial regression
model.bin <- glm(cbind(Survive, Die) ~ scale(Elev), data=silver.dat, family=binomial)
model.bb <- vglm(cbind(Survive, Die) ~ scale(Elev), data=silver.dat, family=betabinomial)

summary(model.bb)
```

---
# Comparing parameter estimates

<br>
.pull-left[
###Binomial model
```{r, message=F, warning=F}
coef(model.bin)
confint(model.bin, method="profile")
```
]
.pull-right[
###Beta-binomial model
```{r}
coef(model.bb)
confintvglm(model.bb, method="profile")
```
]
---
# Comparing model fits

```{r}
print(c(AIC(model.bin), AIC(model.bb)))
```
----
```{r, echo=F, out.width="60%", fig.asp=0.8}
pred.sil.bb <- predict(model.bb, type="response")
ggplot(data=silver.dat, aes(x=Elev, y=Survival)) +
  geom_point(size=3) + 
  geom_rangeframe() + 
  geom_line(y=pred.sil) + 
  geom_line(aes(y=pred.sil.bb), col="cornflowerblue", size=1.2) + 
  xlab("Elevation") + ylab('Proportion surviving')
```

# Exercise 7D
-----
## Fit an overdispersed categorical model

---
#Summary 

- GLM's extend the application of LM's beyond the normal distribution. 
  - Often useful with biological data

- Standard distributions in `glm` cannot handle overdisperison.

- Overdispersion can have strong influences on SE's and p-values
  - Not accounting for overdispersion can lead to overconfidence in estimates and model.

